{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAhWo4JzEMGdfb6GLh79Ez"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-hTSjql9vFe",
        "outputId": "51756c1b-dddb-421c-90d2-1382b266190f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello collabski\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello collabski\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbiunSMt-KTa",
        "outputId": "35861e4c-1f90-487b-9777-a9bd1ba0c642"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "2.0.2\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "vocab_size = 6 #6 token test vocabulary\n",
        "embedding_dim = 128 #vector size per token\n",
        "word_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "#example:\n",
        "input_sentence_ids = torch.tensor([[1,3,4,5]], dtype=torch.long)\n",
        "\n",
        "#move input to gpu\n",
        "if torch.cuda.is_available():\n",
        "  print(\"moving to gpu\")\n",
        "  word_embedding_layer.to('cuda') #moving the whole embed token layer\n",
        "  input_sentence_ids = input_sentence_ids.to('cuda')\n",
        "#pass id's through the embedding layer and then get the shape resutls\n",
        "embedded_sentence = word_embedding_layer(input_sentence_ids)\n",
        "print(f\"sentence ids: {input_sentence_ids.shape}\")\n",
        "print(f\"embed sentenc: {embedded_sentence.shape}\")\n",
        "print(f\"First word's embedding:{embedded_sentence[0,0, :5]}\")\n",
        "print(f\"is tensor in gpu? {embedded_sentence.is_cuda}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2v4oQU0_KUa",
        "outputId": "3198881e-0cfc-4342-8513-3a6b9291c021"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moving to gpu\n",
            "sentence ids: torch.Size([1, 4])\n",
            "embed sentenc: torch.Size([1, 4, 128])\n",
            "First word's embedding:tensor([-0.2959, -0.1306, -0.8948, -0.8562,  0.5636], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "is tensor in gpu? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSxq6GORF5oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}