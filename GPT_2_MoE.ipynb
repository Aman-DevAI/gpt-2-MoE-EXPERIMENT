{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPW2QNly4PYOvRO2F2ZZ4G1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-hTSjql9vFe",
        "outputId": "414a40ab-833b-4f58-8eb0-a8c6edee4f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello collabski\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello collabski\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbiunSMt-KTa",
        "outputId": "b999c1eb-30ca-43a8-97f2-09e231d68907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "2.0.2\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "vocab_size = 6 #6 token test vocabulary\n",
        "embedding_dim = 128 #vector size per token\n",
        "word_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "#example:\n",
        "input_sentence_ids = torch.tensor([[1,3,4,5]], dtype=torch.long)\n",
        "\n",
        "#move input to gpu\n",
        "if torch.cuda.is_available():\n",
        "  print(\"moving to gpu\")\n",
        "  word_embedding_layer.to('cuda') #moving the whole embed token layer\n",
        "  input_sentence_ids = input_sentence_ids.to('cuda')\n",
        "#pass id's through the embedding layer and then get the shape resutls\n",
        "embedded_sentence = word_embedding_layer(input_sentence_ids)\n",
        "print(f\"sentence ids: {input_sentence_ids.shape}\")\n",
        "print(f\"embed sentenc: {embedded_sentence.shape}\")\n",
        "print(f\"First word's embedding:{embedded_sentence[0,0, :5]}\")\n",
        "print(f\"is tensor in gpu? {embedded_sentence.is_cuda}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2v4oQU0_KUa",
        "outputId": "7513f163-38bb-4f24-8680-80321bd3e165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moving to gpu\n",
            "sentence ids: torch.Size([1, 4])\n",
            "embed sentenc: torch.Size([1, 4, 128])\n",
            "First word's embedding:tensor([-1.1455,  0.2516,  0.5178, -0.1356, -0.4431], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "is tensor in gpu? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_features = embedding_dim #input size of embedding layer\n",
        "output_featuers = 256 #transforming weights of the layer from 128 number vectors  to 256 number vector\n",
        "linear_layer = nn.Linear(input_features, output_featuers) #linear changes the weights to find the correct syntax\n",
        "if torch.cuda.is_available():\n",
        "  print(\"moving to gpu\")\n",
        "  linear_layer.to('cuda')\n",
        "\n",
        "output_linear = linear_layer(embedded_sentence)\n",
        "print(f\"input shape: {embedded_sentence.shape}\")\n",
        "print(f\"output shape: {output_linear.shape}\")\n",
        "activation_function = nn.ReLU()\n",
        "output_activation = activation_function(output_linear)\n",
        "print(f\"{output_activation.shape}\")\n",
        "print(f\"{output_activation[0,0, :5]}\")\n",
        "print(f\"{output_activation.is_cuda}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSxq6GORF5oz",
        "outputId": "801ff91c-c369-40e9-9302-0b386948fd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moving to gpu\n",
            "input shape: torch.Size([1, 4, 128])\n",
            "output shape: torch.Size([1, 4, 256])\n",
            "torch.Size([1, 4, 256])\n",
            "tensor([0.2333, 0.0000, 0.1738, 0.7630, 0.0192], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#self attention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "input_dim = 128\n",
        "qkv_dim = input_dim\n",
        "query_linear = nn.Linear(input_dim, qkv_dim, bias = False)\n",
        "key_linear = nn.Linear(input_dim, qkv_dim, bias = False)\n",
        "value_linear = nn.Linear(input_dim, qkv_dim, bias = False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Moving Q,K, V linear layesr to gpu\")\n",
        "  query_linear.to('cuda')\n",
        "  key_linear.to('cuda')\n",
        "  value_linear.to('cuda')\n",
        "\n",
        "Q = query_linear(embedded_sentence)\n",
        "K = key_linear(embedded_sentence)\n",
        "V = value_linear(embedded_sentence)\n",
        "\n",
        "print(f\"\\nShape of Q (Query): {Q.shape}\")\n",
        "print(f\"Shape of K (Key): {K.shape}\")\n",
        "print(f\"Shape of V (Value): {V.shape}\")\n",
        "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) # Shape: [1, 4, 4]\n",
        "\n",
        "print(f\"\\nShape of raw attention scores: {attention_scores.shape}\")\n",
        "print(\"Raw attention scores (example for the single sentence):\\n\", attention_scores[0].detach().cpu().numpy())\n",
        "scaling_factor = torch.sqrt(torch.tensor(qkv_dim, dtype=torch.float32))\n",
        "if torch.cuda.is_available():\n",
        "    scaling_factor = scaling_factor.to('cuda')\n",
        "scaled_attention_scores = attention_scores / scaling_factor\n",
        "\n",
        "print(f\"\\nShape of scaled attention scores: {scaled_attention_scores.shape}\")\n",
        "print(\"Scaled attention scores:\\n\", scaled_attention_scores[0].detach().cpu().numpy())\n",
        "\n",
        "attention_probabilities = F.softmax(scaled_attention_scores, dim=-1)\n",
        "\n",
        "print(f\"\\nShape of attention probabilities: {attention_probabilities.shape}\")\n",
        "print(\"Attention probabilities (each row sums to 1 for the sentence):\\n\", attention_probabilities[0].detach().cpu().numpy())\n",
        "output_attention = torch.matmul(attention_probabilities, V) # Shape: [1, 4, 128]\n",
        "\n",
        "print(f\"\\nShape of output from single head self-attention: {output_attention.shape}\")\n",
        "print(\"Output from self-attention (first word, first 5 numbers, context-rich):\\n\", output_attention[0, 0, :5].detach().cpu().numpy())\n",
        "print(f\"Is the attention output tensor on GPU? {output_attention.is_cuda}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PCmIaHEOkSU",
        "outputId": "5cadc403-27fa-473e-821b-04e365561a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving Q,K, V linear layesr to gpu\n",
            "\n",
            "Shape of Q (Query): torch.Size([1, 4, 128])\n",
            "Shape of K (Key): torch.Size([1, 4, 128])\n",
            "Shape of V (Value): torch.Size([1, 4, 128])\n",
            "\n",
            "Shape of raw attention scores: torch.Size([1, 4, 4])\n",
            "Raw attention scores (example for the single sentence):\n",
            " [[ 2.4522426   1.9550277  -1.9412539   0.33083355]\n",
            " [-4.0718694   4.869228   -8.965159    6.255979  ]\n",
            " [ 1.9293495   4.006773    1.9761771  -1.3404183 ]\n",
            " [-7.675651   -3.1479838   6.284437   -0.5502856 ]]\n",
            "\n",
            "Shape of scaled attention scores: torch.Size([1, 4, 4])\n",
            "Scaled attention scores:\n",
            " [[ 0.21674968  0.17280167 -0.17158423  0.02924183]\n",
            " [-0.3599058   0.43038303 -0.7924156   0.5529557 ]\n",
            " [ 0.17053202  0.35415205  0.17467102 -0.11847737]\n",
            " [-0.6784381  -0.2782451   0.555471   -0.04863884]]\n",
            "\n",
            "Shape of attention probabilities: torch.Size([1, 4, 4])\n",
            "Attention probabilities (each row sums to 1 for the sentence):\n",
            " [[0.2886659  0.27625433 0.19576913 0.23931056]\n",
            " [0.15762049 0.34740075 0.10227655 0.39270222]\n",
            " [0.25286293 0.30382976 0.25391167 0.18939562]\n",
            " [0.12813981 0.19119905 0.4401128  0.24054839]]\n",
            "\n",
            "Shape of output from single head self-attention: torch.Size([1, 4, 128])\n",
            "Output from self-attention (first word, first 5 numbers, context-rich):\n",
            " [-0.13544372 -0.55260885  0.07745597 -0.26215816  0.17377311]\n",
            "Is the attention output tensor on GPU? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        print(f\"\\nInside Attention: Q shape: {q.shape}\")\n",
        "        print(f\"Inside Attention: K shape: {k.shape}\")\n",
        "        print(f\"Inside Attention: V shape: {v.shape}\")\n",
        "\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "        scaled_attention_scores = attention_scores / (self.head_dim ** 0.5)\n",
        "\n",
        "        attention_probs = F.softmax(scaled_attention_scores, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attention_probs, v)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "embed_dim_model = 128\n",
        "num_attention_heads = 8\n",
        "\n",
        "mha_layer = MultiHeadSelfAttention(embed_dim_model, num_attention_heads)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nMoving MultiHeadSelfAttention layer to GPU...\")\n",
        "    mha_layer.to('cuda')\n",
        "\n",
        "\n",
        "print(f\"\\nInput to Multi-Head Attention Layer shape: {embedded_sentence.shape}\")\n",
        "print(f\"Input tensor on GPU? {embedded_sentence.is_cuda}\")\n",
        "\n",
        "output_from_mha = mha_layer(embedded_sentence)\n",
        "\n",
        "print(f\"\\nOutput from Multi-Head Attention Layer shape: {output_from_mha.shape}\")\n",
        "print(f\"Output tensor on GPU? {output_from_mha.is_cuda}\")\n"
      ],
      "metadata": {
        "id": "2M4Kng27jJWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe461a33-09d2-4acd-ff21-873b654ce3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Moving MultiHeadSelfAttention layer to GPU...\n",
            "\n",
            "Input to Multi-Head Attention Layer shape: torch.Size([1, 4, 128])\n",
            "Input tensor on GPU? True\n",
            "\n",
            "Inside Attention: Q shape: torch.Size([1, 8, 4, 16])\n",
            "Inside Attention: K shape: torch.Size([1, 8, 4, 16])\n",
            "Inside Attention: V shape: torch.Size([1, 8, 4, 16])\n",
            "\n",
            "Output from Multi-Head Attention Layer shape: torch.Size([1, 4, 128])\n",
            "Output tensor on GPU? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz, seq_len, _ = x.shape\n",
        "        qkv = self.qkv_proj(x)                        # [b, seq, 3*embed]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)                # each [b, seq, embed]\n",
        "        q = q.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "        scores = scores / (self.head_dim ** 0.5)\n",
        "        probs = F.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(probs, v)              # [b, heads, seq, head_dim]\n",
        "\n",
        "        context = context.transpose(1, 2).contiguous().view(bsz, seq_len, self.embed_dim)\n",
        "        out = self.out_proj(context)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.norm1(x)\n",
        "        y = self.attention(y)\n",
        "        x = x + self.dropout(y)\n",
        "\n",
        "        y = self.norm2(x)\n",
        "        y = self.feed_forward(y)\n",
        "        x = x + self.dropout(y)\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    embed_dim_model = 128\n",
        "    num_attention_heads = 8\n",
        "    ff_hidden_dim = embed_dim_model * 4\n",
        "\n",
        "    transformer_block = TransformerBlock(\n",
        "        embed_dim=embed_dim_model,\n",
        "        num_heads=num_attention_heads,\n",
        "        ff_dim=ff_hidden_dim\n",
        "    )\n",
        "\n",
        "    batch_size, seq_len = 2, 10\n",
        "    embedded_sentence = torch.randn(batch_size, seq_len, embed_dim_model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Moving TransformerBlock to GPU…\")\n",
        "        transformer_block = transformer_block.cuda()\n",
        "        embedded_sentence = embedded_sentence.cuda()\n",
        "\n",
        "    print(\"Input to Transformer Block shape:\", embedded_sentence.shape)\n",
        "    print(\"Input tensor on GPU?\", embedded_sentence.is_cuda)\n",
        "\n",
        "    output_from_transformer_block = transformer_block(embedded_sentence)\n",
        "\n",
        "    print(\"Output from Transformer Block shape:\", output_from_transformer_block.shape)\n",
        "    print(\"Output tensor on GPU?\", output_from_transformer_block.is_cuda)\n",
        "\n"
      ],
      "metadata": {
        "id": "JChqjfUmvZJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96c8037-cd19-48e8-91be-e6c58f4bbe57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving TransformerBlock to GPU…\n",
            "Input to Transformer Block shape: torch.Size([2, 10, 128])\n",
            "Input tensor on GPU? True\n",
            "Output from Transformer Block shape: torch.Size([2, 10, 128])\n",
            "Output tensor on GPU? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4M6zlur4GEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}