# gpt-2-MoE-EXPERIMENT
I aim to learn how the gpt architecture works under the hood while integrating some of the recent advancements such as Mixture-of-Experts (MoE)  to the architecture. 
